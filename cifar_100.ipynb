{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "MODEL_DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.testdriller.com/pictures/blog/57043786ab6fa09.jpg\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR100 Classification\n",
    "\n",
    "> Can we develop a model that performs well on the benchmark dataset CIFAR100?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "The CIFAR-100 dataset (Canadian Institute for Advanced Research) is a subset of the Tiny Images dataset and consists of <strong>60000</strong> <code>32x32</code> colour images in <strong>100</strong> classes, with <strong>600</strong> images per class. The <strong>100</strong> classes in the CIFAR-100 are <strong>grouped into 20 superclasses</strong>. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs). There are <strong>50000 training images</strong> and <strong>10000 test images.</strong>\n",
    "\n",
    "Credit: <a href=\"https://www.kaggle.com/datasets/fedesoriano/cifar100?select=meta\">Kaggle Link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "<ol>\n",
    "\t<li>To explore and understand the CIFAR100 dataset</li>\n",
    "\t<li>Understand the effects of different data augmentation techniques on the performanceo f the model</li>\n",
    "\t<li>Discover new techniques and approaches as to tackle the <strong>3 color-channels (RGB) nature</strong> of the dataset.</li>\n",
    "\t<li>Develop and experiment with models in order to rival state-of-the-art (SOTA) benchmark scores.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "We import the necessary libraries for the notebook to run below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the seed such that the notebook results in reproducible results when run.   \n",
    "We also set the device to CUDA to enable torch to use our GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available now: cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "# When running on the CuDNN backend, two further options must be set\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# Set a fixed value for the hash seed\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device available now:', device)\n",
    "if device != torch.device('cuda'):\n",
    "    print('using cpu, exiting')\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "Below we define some utility functions that will ease and help us with our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc_data(data, loc):\n",
    "\tdatacopy = copy.deepcopy(data)\n",
    "\tarr = np.array(datacopy.loc[loc].drop('label'))\n",
    "\tlabel = datacopy.loc[loc]['label']\n",
    "\troot = int(len(arr) ** 0.5)\n",
    "\tarr.resize((root, root))\n",
    "\treturn label, arr\n",
    "\n",
    "def imshow(arr: list, label: list = None, grayscale=True, figsize=None):\n",
    "\tif label == None:\n",
    "\t\tlabel = [''] * len(arr)\n",
    "\n",
    "\theight = int(len(arr) ** 0.5)\n",
    "\twidth = math.ceil(len(arr) / height)\n",
    "\n",
    "\tif figsize == None:\n",
    "\t\tfig = plt.figure()\n",
    "\telse:\n",
    "\t\tfig = plt.figure(figsize=figsize)\n",
    "\tfor i in range(height):\n",
    "\t\tfor j in range(width):\n",
    "\t\t\tax = fig.add_subplot(height, width, i * height + j + 1)\n",
    "\t\t\tax.grid(False)\n",
    "\t\t\tax.set_xticks([])\n",
    "\t\t\tax.set_yticks([])\n",
    "\t\t\tax.imshow(arr[i * height + j], cmap='gray' if grayscale else '')\n",
    "\t\t\tax.set_title(label[i * height + j])\n",
    "\n",
    "def df_to_tensor(df, shape = (28, 28)):\n",
    "\treturn torch.tensor(df.values.reshape((-1, *shape)), dtype=torch.float32)\n",
    "\n",
    "def preprocess(df):\n",
    "\treturn df.applymap(lambda x: x / 255)\n",
    "\n",
    "def mse(t1, t2, shape=(28, 28)):\n",
    "\tloss = nn.MSELoss(reduction='none')\n",
    "\tloss_result = torch.sum(loss(t1, t2), dim=2)\n",
    "\tloss_result = torch.sum(loss_result, dim=2)\n",
    "\tloss_result = loss_result / np.prod([*shape])\n",
    "\treturn loss_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Let's take a look at the dataset. This dataset was retrieved from the original Fashion MNIST dataset found at <a href=\"https://arxiv.org/pdf/1708.07747v2.pdf\">Paper Link</a>. \n",
    "\n",
    "<table>\n",
    "\t<tr>\n",
    "\t\t<th>\n",
    "\t\t\tColumn Name\n",
    "\t\t</th>\n",
    "\t\t<th>\n",
    "\t\t\tDescription\n",
    "\t\t</th>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>\n",
    "\t\t\tlabel\n",
    "\t\t</td>\n",
    "\t\t<td>\n",
    "\t\t\tThe true class of the image, represented as an integer ranging from 1 to 100<strong>*</strong>\n",
    "\t\t</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>\n",
    "\t\t\tpixel 1<br/>...<br/>pixel 3072\n",
    "\t\t</td>\n",
    "\t\t<td>\n",
    "\t\t\tPixels representing the image, each pixel ranging from 0 to 255. Each image has a dimension of <code>32x32x3</code>.\n",
    "\t\t</td>\n",
    "\t</tr>\n",
    "</table>\n",
    "\n",
    "<strong>\\*</strong>Each number represents a certain dress item\n",
    "```\n",
    "1-5 -> beaver, dolphin, otter, seal, whale\n",
    "6-10 -> aquarium fish, flatfish, ray, shark, trout\n",
    "11-15 -> orchids, poppies, roses, sunflowers, tulips\n",
    "16-20 -> bottles, bowls, cans, cups, plates\n",
    "21-25 -> apples, mushrooms, oranges, pears, sweet peppers\n",
    "26-30 -> clock, computer keyboard, lamp, telephone, television\n",
    "31-35 -> bed, chair, couch, table, wardrobe\n",
    "36-40 -> bee, beetle, butterfly, caterpillar, cockroach\n",
    "41-45 -> bear, leopard, lion, tiger, wolf\n",
    "46-50 -> bridge, castle, house, road, skyscraper\n",
    "51-55 -> cloud, forest, mountain, plain, sea\n",
    "56-60 -> camel, cattle, chimpanzee, elephant, kangaroo\n",
    "61-65 -> fox, porcupine, possum, raccoon, skunk\n",
    "66-70 -> crab, lobster, snail, spider, worm\n",
    "71-75 -> baby, boy, girl, man, woman\n",
    "76-80 -> crocodile, dinosaur, lizard, snake, turtle\n",
    "81-85 -> hamster, mouse, rabbit, shrew, squirrel\n",
    "86-90 -> maple, oak, palm, pine, willow\n",
    "91-95 -> bicycle, bus, motorcycle, pickup truck, train\n",
    "96-100 -> lawn-mower, rocket, streetcar, tank, tractor\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "metadata_path = 'data/meta' # change this path\n",
    "metadata = unpickle(metadata_path)\n",
    "superclass_dict = dict(list(enumerate(metadata[b'coarse_label_names'])))\n",
    "\n",
    "data_pre_path = 'data/' # change this path\n",
    "# File paths\n",
    "data_train_path = data_pre_path + 'train'\n",
    "data_test_path = data_pre_path + 'test'\n",
    "# Read dictionary\n",
    "data_train_dict = unpickle(data_train_path)\n",
    "data_test_dict = unpickle(data_test_path)\n",
    "# Get data (change the coarse_labels if you want to use the 100 classes)\n",
    "X_train = data_train_dict[b'data']\n",
    "y_train = np.array(data_train_dict[b'fine_labels'])\n",
    "X_test = data_test_dict[b'data']\n",
    "y_test = np.array(data_test_dict[b'fine_labels'])\n",
    "\n",
    "classes = np.array(list(map(lambda x: x.decode('utf-8'), metadata[b'fine_label_names'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3072)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there are a total of <code>50000</code> rows and <code>3072</code> columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for missing values and invalid ata\n",
    "Let's try to identify if there are any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature missing values: 0\n",
      "Feature missing values: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature missing values:\",pd.DataFrame(X_train).isnull().sum().sum())\n",
    "print(\"Feature missing values:\",pd.DataFrame(y_train).isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = np.transpose(data_test.reshape((-1, 3, 32, 32)), axes=[0,2,3,1])\n",
    "# plt.imshow(temp[0])\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d19efe0496419facc97fd7ea20d5d3a0b9c1bdc9473c8f87e1305d92cacb204"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
